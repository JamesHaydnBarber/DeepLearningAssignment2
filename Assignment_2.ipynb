{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "import seaborn as sns\n",
        "\n",
        "#Set the seed for better comparison\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)"
      ],
      "metadata": {
        "id": "kjiOqimp39w_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load CIFAR 10 dataset"
      ],
      "metadata": {
        "id": "jDMopdZR3W3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "#Get the CIFAR datasets\n",
        "train_data = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "\n",
        "train, validation = random_split(train_data, [int(0.8 * len(train_data)), len(train_data) - int(0.8 * len(train_data))])\n",
        "\n",
        "\n",
        "trainloader = DataLoader(train, batch_size=128)\n",
        "valloader = DataLoader(validation, batch_size=128,)\n",
        "testloader = DataLoader(test, batch_size=128)\n",
        "\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "print(\"Train Size: \"+str(len(train)))\n",
        "print(\"Validation Size: \"+str(len(validation)))\n",
        "print(\"Test Size: \"+str(len(test)))"
      ],
      "metadata": {
        "id": "vodF7ptQBor1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ResNet-18"
      ],
      "metadata": {
        "id": "yg3RdJRb3lYB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define model"
      ],
      "metadata": {
        "id": "nZEfBKbD4Eqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def trainResNet18(learningRate, optimizerName, Residual, patience=3, epochs=15):\n",
        "\n",
        "  #Initialize the resnet 18 model, setting the pretrained weifghts to false, ensuring the model has not seen the CIFAR-10 dataset before\n",
        "  model = models.resnet18(pretrained=False)\n",
        "\n",
        "  #This portion of the code removes the residual links if that has been selected for the gridsearch\n",
        "  if not Residual:\n",
        "    for layer in model.children():\n",
        "      if isinstance(layer, models.resnet.BasicBlock):\n",
        "        layer.downsample = None\n",
        "        layer.conv2 = nn.Conv2d(layer.conv2.in_channels, layer.conv2.out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "\n",
        "  #Define the model, setting the fully connected layer to 10 to handle the 10 classes in the CIFAR-10 dataset. The fully connected layer is the layer that will ultimately decide what class a input belongs, so it needs to have the same number of nodes as the number of classes\n",
        "  model.fc = nn.Linear(model.fc.in_features, 10)\n",
        "\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model.to(device)\n",
        "\n",
        "  #Define the loss function and set up the optomizer based on which has been passed to the model from the gridsearch. CrossEntropyLoss also known as log loss is being used in these models because it works well in stochastic gradient descent optimizers like the ones i have chosen below. The optimizers also needs to define the learning rates, which have also been passed to it\n",
        "  lossFunction = nn.CrossEntropyLoss()\n",
        "  if optimizerName == 'Adam':\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learningRate)\n",
        "  elif optimizerName == 'SGD':\n",
        "    optimizer = optim.SGD(model.parameters(), lr=learningRate, momentum=0.9)\n",
        "\n",
        "  #Here i am defining some variable to store the metrics so that i can plot them later\n",
        "  bestValidationAccuracy = 0\n",
        "  epochsNoImprove = 0\n",
        "  trainingLosses, validationAccuracies = [], []\n",
        "  trainingAccuracies = []\n",
        "\n",
        "  #The for loop is constrained by the number of epochs. IN each epoch the model trains on the images and labels, calculates the metrics and then validates the model on the validation data set. i make sure that the test set is not seen by the model here so that the training is not infected\n",
        "  for epoch in range(epochs):\n",
        "    print(\"Epoch: \", epoch)\n",
        "    model.train()\n",
        "    runningLoss = 0.0\n",
        "    correctTrain = 0\n",
        "    totalTrain = 0\n",
        "\n",
        "    #For each batch of image and label in the trainloader the model is loading it to the gpu for faster training, it then sets the gradients to zero to avoid accumulation. It then predicts the labels for the images, storing the results in the outputs. The loss is then calculated using the loss that was defined earlier ...\n",
        "    #The loss is calculated using by comparing the outputs/ predictions made by the model on the training set and the true labels. This essentially calculates the errors. The loss gradients is then used in the backpropogation step, which updates all the weights in the optimal direction\n",
        "    for images, labels in trainloader:\n",
        "      images, labels = images.to(device), labels.to(device)\n",
        "      optimizer.zero_grad()\n",
        "      outputs = model(images)\n",
        "      loss = lossFunction(outputs, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      #I will now calculate all of the metrics for the batch\n",
        "      runningLoss = runningLoss + loss.item()\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      totalTrain = totalTrain + labels.size(0)\n",
        "      correctTrain = correctTrain + (predicted == labels).sum().item()\n",
        "\n",
        "    #Calculate all the metrics for this epoch, store them so that i can plot them later and analyse what happened during the training process\n",
        "    epochLoss = runningLoss / len(trainloader)\n",
        "    trainingLosses.append(epochLoss)\n",
        "    trainAccuracy = correctTrain / totalTrain\n",
        "    trainingAccuracies.append(trainAccuracy)\n",
        "\n",
        "    #Using a seperate validation set so that i dont infect the training data i need to validate the models performance. The model technically has not been trained on the validation set so this is unseen data, however because the model will be adjusted based on the results of this validation set the model will fit to this dataset thus infecting it.\n",
        "    model.eval()\n",
        "    correctVal = 0\n",
        "    totalVal = 0\n",
        "    with torch.no_grad():\n",
        "      for images, labels in valloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        totalVal = totalVal + labels.size(0)\n",
        "        correctVal = correctVal + (predicted == labels).sum().item()\n",
        "\n",
        "    validationAccuracy = correctVal / totalVal\n",
        "    validationAccuracies.append(validationAccuracy)\n",
        "\n",
        "    #Apply early stopping by checking if the validation acccuracy is not improving for the length of the patience. Also if the model is has the best validation accuracy save the weights to use later to avoid having to retrain. This is to improve computational efficiency as there is no point in continuing the training of a model that is not performing well\n",
        "    if validationAccuracy > bestValidationAccuracy:\n",
        "      bestValidationAccuracy = validationAccuracy\n",
        "      torch.save(model.state_dict(), 'best_ResNet18.pth')\n",
        "      epochsNoImprove = 0\n",
        "    else:\n",
        "      epochsNoImprove = epochsNoImprove + 1\n",
        "      if epochsNoImprove >= patience:\n",
        "        print(f\"Early stopping at epoch {epoch+1}\")\n",
        "        break\n",
        "\n",
        "  return trainingLosses, trainingAccuracies, validationAccuracies, bestValidationAccuracy\n",
        "\n",
        "#Define the paramters to use in the grid search, i have not used very many due to resource limitations as it takes around 30 minutes to train one iteration of the params\n",
        "learningRates = [0.01, 0.001]\n",
        "optimizers = ['Adam', 'SGD']\n",
        "residuals = [True, False]\n",
        "\n",
        "bestAccuracy = 0\n",
        "bestParams = {}\n",
        "allTrainingLosses = {}\n",
        "allValidationAccuracies = {}\n",
        "\n",
        "#Perform the gridsearch by iterating through all of the options using itertools to ensure every combination of hyperparameters is covered. Store the params of the model with the best validation accuracy, as this will likely be the best perfrorming model.\n",
        "for lr, opt, residual in itertools.product(learningRates, optimizers, residuals):\n",
        "  trainLosses, trainAccuracies, valAccuracies, valAccuracy = trainResNet18(lr, opt, residual, patience=3)\n",
        "\n",
        "  if valAccuracy > bestAccuracy:\n",
        "    bestAccuracy = valAccuracy\n",
        "    bestParams = {'learning_rate': lr, 'optimizer': opt, 'use_residual': residual}\n",
        "\n",
        "  paramStr = f\"lr={lr}, opt={opt}, residual={residual}\"\n",
        "  allTrainingLosses[paramStr] = trainLosses\n",
        "  allValidationAccuracies[paramStr] = valAccuracies"
      ],
      "metadata": {
        "id": "MT48J8YbE1IT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Best Validation Accuracy:\", bestAccuracy)\n",
        "print(\"Best Hyperparameters:\", bestParams)\n",
        "\n",
        "# Plot all grid search training loss curves\n",
        "plt.figure(figsize=(12, 6))\n",
        "for key, losses in allTrainingLosses.items():\n",
        "  plt.plot(losses, label=key)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Training Loss\")\n",
        "plt.title(\"Training Loss ResNet-18\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot all grid search validation accuracy curves\n",
        "plt.figure(figsize=(12, 6))\n",
        "for key, accuracies in allValidationAccuracies.items():\n",
        "  plt.plot(accuracies, label=key)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Validation Accuracy\")\n",
        "plt.title(\"Validation Accuracy for ResNet-18\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-5wGT8uTEsLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluateResNet18(testloader):\n",
        "  # Load the best model weights\n",
        "  model = models.resnet18(pretrained=False)\n",
        "  model.fc = nn.Linear(model.fc.in_features, 10)\n",
        "  model.load_state_dict(torch.load('best_ResNet18.pth'))\n",
        "  model.eval()\n",
        "\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model.to(device)\n",
        "\n",
        "  predictions = []\n",
        "  pLabels = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for images, labels in testloader:\n",
        "      images, labels = images.to(device), labels.to(device)\n",
        "      outputs = model(images)\n",
        "      _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "      predictions.extend(predicted.cpu().numpy())\n",
        "      pLabels.extend(labels.cpu().numpy())\n",
        "\n",
        "  # Calculate overall accuracy\n",
        "  testAccuracy = accuracy_score(pLabels, predictions)\n",
        "  print(f\"Test Accuracy: {testAccuracy:.4f}\")\n",
        "\n",
        "  # Confusion Matrix\n",
        "  cm = confusion_matrix(pLabels, predictions)\n",
        "  plt.figure(figsize=(8, 6))\n",
        "  sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=classes, yticklabels=classes)\n",
        "  plt.xlabel(\"Predicted Label\")\n",
        "  plt.ylabel(\"True Label\")\n",
        "  plt.title(\"Confusion Matrix for ResNet-18\")\n",
        "  plt.show()\n",
        "\n",
        "  # Classification Report\n",
        "  print(\"Classification Report for ResNet18:\")\n",
        "  print(classification_report(pLabels, predictions))\n",
        "\n",
        "# Assuming `testloader` is the DataLoader for your test dataset\n",
        "evaluateResNet18(testloader)"
      ],
      "metadata": {
        "id": "GSgJf_LL4P01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AlexNet"
      ],
      "metadata": {
        "id": "cQLpK-3H6iff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def trainAlexNet(learningRate, optimizerName, weight_decay, patience=3, epochs=15):\n",
        "  model = models.alexnet(pretrained=False)\n",
        "  model.features[0] = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "  model.classifier[6] = nn.Linear(model.classifier[6].in_features, 10)\n",
        "\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model.to(device)\n",
        "\n",
        "  lossFunction = nn.CrossEntropyLoss()\n",
        "  if optimizerName == 'Adam':\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learningRate)\n",
        "  elif optimizerName == 'SGD':\n",
        "    optimizer = optim.SGD(model.parameters(), lr=learningRate, momentum=0.9)\n",
        "\n",
        "  bestValidationAccuracy = 0\n",
        "  epochsNoImprove = 0\n",
        "  trainingLosses, validationAccuracies = [], []\n",
        "  trainingAccuracies = []\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    print(\"Epoch: \" + str(epoch))\n",
        "    model.train()\n",
        "    runningLoss = 0.0\n",
        "    correctTrain = 0\n",
        "    totalTrain = 0\n",
        "\n",
        "    for images, labels in trainloader:\n",
        "      images, labels = images.to(device), labels.to(device)\n",
        "      optimizer.zero_grad()\n",
        "      outputs = model(images)\n",
        "      loss = lossFunction(outputs, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      runningLoss = runningLoss + loss.item()\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      totalTrain = totalTrain + labels.size(0)\n",
        "      correctTrain = correctTrain + (predicted == labels).sum().item()\n",
        "\n",
        "    epochLoss = runningLoss / len(trainloader)\n",
        "    trainingLosses.append(epochLoss)\n",
        "    trainAccuracy = correctTrain / totalTrain\n",
        "    trainingAccuracies.append(trainAccuracy)\n",
        "\n",
        "    model.eval()\n",
        "    correctVal = 0\n",
        "    totalVal = 0\n",
        "    with torch.no_grad():\n",
        "      for images, labels in valloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        totalVal = totalVal + labels.size(0)\n",
        "        correctVal = correctVal + (predicted == labels).sum().item()\n",
        "\n",
        "    validationAccuracy = correctVal / totalVal\n",
        "    validationAccuracies.append(validationAccuracy)\n",
        "\n",
        "    if validationAccuracy > bestValidationAccuracy:\n",
        "      bestValidationAccuracy = validationAccuracy\n",
        "      torch.save(model.state_dict(), 'best_AlexNet.pth')\n",
        "      epochsNoImprove = 0\n",
        "    else:\n",
        "      epochsNoImprove = epochsNoImprove + 1\n",
        "      if epochsNoImprove >= patience:\n",
        "        print(\"Early stopping at epoch \" + str(epoch+1))\n",
        "        break\n",
        "\n",
        "  return trainingLosses, trainingAccuracies, validationAccuracies, bestValidationAccuracy\n",
        "\n",
        "learningRates = [0.01, 0.001]\n",
        "optimizers = ['Adam', 'SGD']\n",
        "weightDecay = [1e-2,1e-4]\n",
        "\n",
        "bestAccuracy = 0\n",
        "bestParams = {}\n",
        "allTrainingLosses = {}\n",
        "allValidationAccuracies = {}\n",
        "\n",
        "for lr, opt, wd in itertools.product(learningRates, optimizers, weightDecay):\n",
        "  print(f\"Training with lr={lr}, optimizer={opt}, weight_decay={wd}\")\n",
        "  trainLosses, trainAccuracies, valAccuracies, valAccuracy = trainAlexNet(lr, opt, wd, patience=3)\n",
        "\n",
        "  if valAccuracy > bestAccuracy:\n",
        "    bestAccuracy = valAccuracy\n",
        "    bestParams = {'learning_rate': lr, 'optimizer': opt, 'weight_decay': wd}\n",
        "\n",
        "  paramStr = f\"lr={lr}, opt={opt}, weight_decay={wd}\"\n",
        "  allTrainingLosses[paramStr] = trainLosses\n",
        "  allValidationAccuracies[paramStr] = valAccuracies"
      ],
      "metadata": {
        "id": "QBIz5qw9FM64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Best Validation Accuracy:\", bestAccuracy)\n",
        "print(\"Best Hyperparameters:\", bestParams)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "for key, losses in allTrainingLosses.items():\n",
        "    plt.plot(losses, label=key)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Training Loss\")\n",
        "plt.title(\"Training Loss for Different Hyperparameter Combinations\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "s\n",
        "plt.figure(figsize=(12, 6))\n",
        "for key, accuracies in allValidationAccuracies.items():\n",
        "    plt.plot(accuracies, label=key)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Validation Accuracy\")\n",
        "plt.title(\"Validation Accuracy for Different Hyperparameter Combinations\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Z4CXC-iuQZFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluateAlexNet(testloader):\n",
        "  model = models.alexnet(pretrained=False)\n",
        "  model.features[0] = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "  #There are 10 classes in CIFAR-10 so ensure this is reflected in the model\n",
        "  model.classifier[6] = nn.Linear(model.classifier[6].in_feapredictions)\n",
        "  model.load_state_dict(torch.load('best_AlexNet.pth'))\n",
        "  model.eval()\n",
        "\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model.to(device)\n",
        "\n",
        "  predictions = []\n",
        "  all_labels = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for images, labels in testloader:\n",
        "      images, labels = images.to(device), labels.to(device)\n",
        "      outputs = model(images)\n",
        "      _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "      predictions.extend(predicted.cpu().numpy())\n",
        "      all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "  test_accuracy = accuracy_score(all_labels, predictions)\n",
        "  print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "  cm = confusion_matrix(all_labels, predictions)\n",
        "  plt.figure(figsize=(8, 6))\n",
        "  sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=classes, yticklabels=classes)\n",
        "  plt.xlabel(\"Predicted Label\")\n",
        "  plt.ylabel(\"True Label\")\n",
        "  plt.title(\"Confusion Matrix for AlexNet\")\n",
        "  plt.show()\n",
        "\n",
        "  print(\"Classification Report for AlexNet:\")\n",
        "  print(classification_report(all_labels, predictions))\n",
        "\n",
        "evaluateAlexNet(testloader)"
      ],
      "metadata": {
        "id": "QFd6_ifwHPde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MobileNet"
      ],
      "metadata": {
        "id": "koLn2gw02tk-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def trainMobileNet(learningRate, optimizerName, dropout, patience=3, epochs=15):\n",
        "  model = models.mobilenet_v2(pretrained=False)\n",
        "\n",
        "  model.classifier = nn.Sequential(\n",
        "      nn.Dropout(p=dropout),\n",
        "      nn.Linear(model.classifier[1].in_features, 10)\n",
        "  )\n",
        "\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model.to(device)\n",
        "\n",
        "  lossFunction = nn.CrossEntropyLoss()\n",
        "  if optimizerName == 'Adam':\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learningRate)\n",
        "  elif optimizerName == 'SGD':\n",
        "    optimizer = optim.SGD(model.parameters(), lr=learningRate, momentum=0.9)\n",
        "\n",
        "  bestValidationAccuracy = 0\n",
        "  epochsNoImprove = 0\n",
        "  trainingLosses, validationAccuracies = [], []\n",
        "  trainingAccuracies = []\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    print(\"Epoch: \" + str(epoch))\n",
        "    model.train()\n",
        "    runningLoss = 0.0\n",
        "    correctTrain = 0\n",
        "    totalTrain = 0\n",
        "\n",
        "    for images, labels in trainloader:\n",
        "      images, labels = images.to(device), labels.to(device)\n",
        "      optimizer.zero_grad()\n",
        "      outputs = model(images)\n",
        "      loss = lossFunction(outputs, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      runningLoss = runningLoss + loss.item()\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      totalTrain = totalTrain + labels.size(0)\n",
        "      correctTrain = correctTrain + (predicted == labels).sum().item()\n",
        "\n",
        "    epochLoss = runningLoss / len(trainloader)\n",
        "    trainingLosses.append(epochLoss)\n",
        "    trainAccuracy = correctTrain / totalTrain\n",
        "    trainingAccuracies.append(trainAccuracy)\n",
        "\n",
        "    model.eval()\n",
        "    correctVal = 0\n",
        "    totalVal = 0\n",
        "    with torch.no_grad():\n",
        "      for images, labels in valloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        totalVal = totalVal + labels.size(0)\n",
        "        correctVal = correctVal + (predicted == labels).sum().item()\n",
        "\n",
        "    validationAccuracy = correctVal / totalVal\n",
        "    validationAccuracies.append(validationAccuracy)\n",
        "\n",
        "    if validationAccuracy > bestValidationAccuracy:\n",
        "      bestValidationAccuracy = validationAccuracy\n",
        "      torch.save(model.state_dict(), 'best_ResNet18.pth')\n",
        "      epochsNoImprove = 0\n",
        "    else:\n",
        "      epochsNoImprove = epochsNoImprove + 1\n",
        "      if epochsNoImprove >= patience:\n",
        "        print(\"Early stopping at epoch \" + str(epoch+1))\n",
        "        break\n",
        "\n",
        "  return trainingLosses, trainingAccuracies, validationAccuracies, bestValidationAccuracy\n",
        "\n",
        "\n",
        "learningRates = [0.01,0.001]\n",
        "optimizers = ['Adam', 'SGD']\n",
        "dropouts = [0.25,0.3]\n",
        "\n",
        "\n",
        "bestAccuracy = 0\n",
        "bestParams = {}\n",
        "allTrainingLosses = {}\n",
        "allValidationAccuracies = {}\n",
        "\n",
        "for lr, opt, dropout in itertools.product(learningRates, optimizers, dropouts):\n",
        "  print(f\"Training with lr={lr}, optimizer={opt}, dropout={dropout}\")\n",
        "  trainLosses, trainAccuracies, valAccuracies, valAccuracy = trainMobileNet(lr, opt, dropout, patience=3)\n",
        "\n",
        "  if valAccuracy > bestAccuracy:\n",
        "    bestAccuracy = valAccuracy\n",
        "    bestParams = {'learning_rate': lr, 'optimizer': opt, 'dropout': dropout}\n",
        "\n",
        "  paramStr = f\"lr={lr}, opt={opt}, dropout={dropout}\"\n",
        "  allTrainingLosses[paramStr] = trainLosses\n",
        "  allValidationAccuracies[paramStr] = valAccuracies"
      ],
      "metadata": {
        "id": "7YMYthqcF-2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Best Validation Accuracy:\", bestAccuracy)\n",
        "print(\"Best Hyperparameters:\", bestParams)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "for key, losses in allTrainingLosses.items():\n",
        "  plt.plot(losses, label=key)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Training Loss\")\n",
        "plt.title(\"Training Loss for MobileNet\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "for key, accuracies in allValidationAccuracies.items():\n",
        "  plt.plot(accuracies, label=key)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Validation Accuracy\")\n",
        "plt.title(\"Validation Accuracy for MobileNet\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cczLvSYj0eyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluateMobileNet(testloader):\n",
        "  #Use the best weights\n",
        "  model = models.mobilenet_v2(pretrained=False)\n",
        "  model.classifier[1] = nn.Linear(model.classifier[1].in_features, 10)\n",
        "  model.load_state_dict(torch.load('best_MobileNet.pth'))\n",
        "  model.eval()\n",
        "\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model.to(device)\n",
        "\n",
        "  predictions = []\n",
        "  pLabels = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for images, labels in testloader:\n",
        "      images, labels = images.to(device), labels.to(device)\n",
        "      outputs = model(images)\n",
        "      _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "      predictions.extend(predicted.cpu().numpy())\n",
        "      pLabels.extend(labels.cpu().numpy())\n",
        "\n",
        "  test_accuracy = accuracy_score(pLabels, predictions)\n",
        "  print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "  cm = confusion_matrix(pLabels, predictions)\n",
        "  plt.figure(figsize=(8, 6))\n",
        "  sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=classes, yticklabels=classes)\n",
        "  plt.xlabel(\"Predicted Label\")\n",
        "  plt.ylabel(\"True Label\")\n",
        "  plt.title(\"Confusion Matrix for MobileNet\")\n",
        "  plt.show()\n",
        "\n",
        "  print(\"Classification Report for MobileNet:\")\n",
        "  print(classification_report(pLabels, predictions))\n",
        "\n",
        "evaluateMobileNet(testloader)"
      ],
      "metadata": {
        "id": "maxd7kqO3JIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Augmentation experiment with ResNet-18"
      ],
      "metadata": {
        "id": "dPhSV9VZqNwt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(20),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_data = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train, validation = random_split(train_data, [int(0.8 * len(train_data)), len(train_data) - int(0.8 * len(train_data))])\n",
        "\n",
        "trainloader = DataLoader(train, batch_size=128, shuffle=True)\n",
        "valloader = DataLoader(validation, batch_size=128, shuffle=False)\n",
        "testloader = DataLoader(test, batch_size=128, shuffle=False)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "print(\"Train Size: \"+str(len(train)))\n",
        "print(\"Validation Size: \"+str(len(validation)))\n",
        "print(\"Test Size: \"+str(len(test)))"
      ],
      "metadata": {
        "id": "K_QRcObkqSrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trainResNet18(learningRate, optimizerName, Residual, patience=3, epochs=15):\n",
        "  model = models.resnet18(pretrained=False)\n",
        "\n",
        "  if not Residual:\n",
        "    for layer in model.children():\n",
        "      if isinstance(layer, models.resnet.BasicBlock):\n",
        "        layer.downsample = None\n",
        "        layer.conv2 = nn.Conv2d(layer.conv2.in_channels, layer.conv2.out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "\n",
        "  model.fc = nn.Linear(model.fc.in_features, 10)\n",
        "\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model.to(device)\n",
        "\n",
        "  lossFunction = nn.CrossEntropyLoss()\n",
        "  if optimizerName == 'Adam':\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learningRate)\n",
        "  elif optimizerName == 'SGD':\n",
        "    optimizer = optim.SGD(model.parameters(), lr=learningRate, momentum=0.9)\n",
        "\n",
        "  bestValidationAccuracy = 0\n",
        "  epochsNoImprove = 0\n",
        "  trainingLosses, validationAccuracies = [], []\n",
        "  trainingAccuracies = []\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    print(\"Epoch: \" + str(epoch))\n",
        "    model.train()\n",
        "    runningLoss = 0.0\n",
        "    correctTrain = 0\n",
        "    totalTrain = 0\n",
        "\n",
        "    for images, labels in trainloader:\n",
        "      images, labels = images.to(device), labels.to(device)\n",
        "      optimizer.zero_grad()\n",
        "      outputs = model(images)\n",
        "      loss = lossFunction(outputs, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      runningLoss = runningLoss + loss.item()\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      totalTrain = totalTrain + labels.size(0)\n",
        "      correctTrain = correctTrain + (predicted == labels).sum().item()\n",
        "\n",
        "    epochLoss = runningLoss / len(trainloader)\n",
        "    trainingLosses.append(epochLoss)\n",
        "    trainAccuracy = correctTrain / totalTrain\n",
        "    trainingAccuracies.append(trainAccuracy)\n",
        "\n",
        "    model.eval()\n",
        "    correctVal = 0\n",
        "    totalVal = 0\n",
        "    with torch.no_grad():\n",
        "      for images, labels in valloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        totalVal = totalVal + labels.size(0)\n",
        "        correctVal = correctVal + (predicted == labels).sum().item()\n",
        "\n",
        "    validationAccuracy = correctVal / totalVal\n",
        "    validationAccuracies.append(validationAccuracy)\n",
        "\n",
        "    if validationAccuracy > bestValidationAccuracy:\n",
        "      bestValidationAccuracy = validationAccuracy\n",
        "      torch.save(model.state_dict(), 'best_ResNet18.pth')\n",
        "      epochsNoImprove = 0\n",
        "    else:\n",
        "      epochsNoImprove = epochsNoImprove + 1\n",
        "      if epochsNoImprove >= patience:\n",
        "        print(\"Early stopping at epoch \" + str(epoch+1))\n",
        "        break\n",
        "\n",
        "  return trainingLosses, trainingAccuracies, validationAccuracies, bestValidationAccuracy\n",
        "\n",
        "learningRates = [0.01, 0.001]\n",
        "optimizers = ['Adam', 'SGD']\n",
        "residuals = [True, False]\n",
        "\n",
        "bestAccuracy = 0\n",
        "bestParams = {}\n",
        "allTrainingLosses = {}\n",
        "allValidationAccuracies = {}\n",
        "\n",
        "for lr, opt, residual in itertools.product(learningRates, optimizers, residuals):\n",
        "  trainLosses, trainAccuracies, valAccuracies, valAccuracy = trainResNet18(lr, opt, residual, patience=3)\n",
        "\n",
        "  if valAccuracy > bestAccuracy:\n",
        "    bestAccuracy = valAccuracy\n",
        "    bestParams = {'learning_rate': lr, 'optimizer': opt, 'use_residual': residual}\n",
        "\n",
        "  paramStr = f\"lr={lr}, opt={opt}, residual={residual}\"\n",
        "  allTrainingLosses[paramStr] = trainLosses\n",
        "  allValidationAccuracies[paramStr] = valAccuracies"
      ],
      "metadata": {
        "id": "HuorA-iqrRNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Best Validation Accuracy:\", bestAccuracy)\n",
        "print(\"Best Hyperparameters:\", bestParams)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "for key, losses in allTrainingLosses.items():\n",
        "  plt.plot(losses, label=key)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Training Loss\")\n",
        "plt.title(\"Training Loss ResNet-18 with Augmentation\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "for key, accuracies in allValidationAccuracies.items():\n",
        "  plt.plot(accuracies, label=key)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Validation Accuracy\")\n",
        "plt.title(\"Validation Accuracy for ResNet-18 with Augmentation\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cFbvNrK8rWof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluateResNet18(testloader):\n",
        "  model = models.resnet18(pretrained=False)\n",
        "  model.fc = nn.Linear(model.fc.in_features, 10)\n",
        "  model.load_state_dict(torch.load('best_ResNet18.pth'))\n",
        "  model.eval()\n",
        "\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model.to(device)\n",
        "\n",
        "  predictions = []\n",
        "  pLabels = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for images, labels in testloader:\n",
        "      images, labels = images.to(device), labels.to(device)\n",
        "      outputs = model(images)\n",
        "      _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "      predictions.extend(predicted.cpu().numpy())\n",
        "      pLabels.extend(labels.cpu().numpy())\n",
        "\n",
        "  testAccuracy = accuracy_score(pLabels, predictions)\n",
        "  print(f\"Test Accuracy: {testAccuracy:.4f}\")\n",
        "\n",
        "  cm = confusion_matrix(pLabels, predictions)\n",
        "  plt.figure(figsize=(8, 6))\n",
        "  sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=classes, yticklabels=classes)\n",
        "  plt.xlabel(\"Predicted Label\")\n",
        "  plt.ylabel(\"True Label\")\n",
        "  plt.title(\"Confusion Matrix for ResNet-18 with Augmentation\")\n",
        "  plt.show()\n",
        "\n",
        "  print(\"Classification Report for ResNet18 with Augmentation:\")\n",
        "  print(classification_report(pLabels, predictions))\n",
        "\n",
        "evaluateResNet18(testloader)"
      ],
      "metadata": {
        "id": "-ZbAczmsrZrs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}